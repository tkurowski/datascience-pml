---
title: "Untitled"
author: "Tomasz Kurowski"
date: "May 9, 2016"
output: html_document
---


<style>
    .side-note {
        font-size: 0.9em; padding: 1em 2em;
        border: solid 1px #ddd;
        border-width: 1px 0;
        margin: 2em 4em;
    }
    .side-note:before {
        content: "side note "; text-transform: uppercase;
        font-size: 0.8em;
    }
    .side-note h5 {
        font-size: 1em; font-weight: bold;
    }

    figure {font-style: italic; margin-bottom: 1em;}
</style>

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, fig.path='plots/',comment=NA,cache=TRUE)
```

```{r, echo=FALSE}
# go to project dir
if (getwd() != "/home/tkurowski/Coursera/PML") setwd("~/Coursera/PML/")
```

## Executive Summary

The full `R` code can be found at LINK_TO_REPO and is considered an integral
part of this report.

## Getting the Data

```{r, echo=FALSE}
for (fname in c("pml-training.csv", "pml-testing.csv"))
    if (!file.exists(fname)) {
        path <- paste("https://d396qusza40orc.cloudfront.net/predmachlearn", fname,
                  sep="/")
        print(fname)
        download.file(path, fname)
    }
```
```{r}
df <- read.csv("pml-training.csv")
```

## Cleaning

```{r withna}
with.na <- sapply(names(df), function (n) sum(is.na(df[[n]]))) # count NA
with.na <- with.na[with.na > 0] # filter columns with NA
```

`r length(with.na)` columns in the data are `r round(min(with.na)/dim(df)[1], 3)*100`%
`NA` (missing values).

<div class="side-note">

##### Assesing the irrelevance of the (almost) empty columns

Being almost empty does not, by itself, make the columns useless
(what if the few available values happen to determine the `classe` perfectly?).
Figure 1. shows the classification tree built using the near-empty columns.

```{r rpart, echo=FALSE}
library(caret)
set.seed(123)
m.with.na <- train(classe ~ .,
                   data = df[, c('classe', names(with.na))],
                   method="rpart")
```

<figure>
```{r rpart_tree, echo=FALSE, fig.width=4, fig.height=2.6}
par('mar', 'cex') -> pars
par(mar=c(0,0,0,0), cex=.75)
plot(m.with.na$finalModel, margin=.2); text(m.with.na$finalModel)
par(pars)
```
  <figcaption>
  Fig. 1. Classification tree built with `rpart` on near-empty columns.
  </figcaption>
</figure>
Only two of them are of any use and the estimated accuracy is only 
`r round(m.with.na$results[1,2], 3)`.
Besides, the tree can only be applied to records where given columns
have proper values.

</div>

We discard irrelevant columns. These include date and time variables,
row number (`X`), `new_window` (yes-no factor that is "no"
`r round(sum(df$new_window == 'no')/dim(df)[1] * 100)`% of time) and `user_name`.

```{r irrelevant}
irrelevant <- c(names(with.na), 'X', 'user_name', 'new_window',
                'cvtd_timestamp', 'raw_timestamp_part_1', 'raw_timestamp_part_2')
df[irrelevant] <- NULL
```

<div class="side-note">
##### Is `user_name` (ir)relevant?

In theory, `user_name` might be a powerful decision variable for the data set.
However, this would yield a very narrow model, unusable for greater audience.
</div>

`read.csv` coerced many variables to factors. We bring them back to their
original numeric domain.

```{r clean}
# NOTE: this clean function must also be applied to the test set (for prediction)
clean <- function (df) {
    for (n in names(df))
        if (n != "classe" && class(df[[n]]) == 'factor')
            df[[n]] <- as.numeric(df[[n]])
    df
}
df <- clean(df)
```

## Partitioning

The `caret` package uses cross-validation to find the best fit; we will additionally
simulate a test set to better evaluate model accuracy.

```{r partition}
set.seed(4321)
intr <- createDataPartition(df$classe, p=0.7, list=FALSE)
tr <- df[intr, ]
tst <- df[-intr, ]
```

## Preliminary Feature Selection

The training dataset now contains `r dim(tr)[1]` rows, `r length(tr)` columns each.
To train such a big data set (with `caret`'s built in cross-validation/bootstrap,
and feature selection) one needs significant amout of time or computing power.
We will therfore perform a very simple feature selection that will, nevertheless,
turn out quite effective.

The idea is to fit a multinomial linear regression model using all variables
and then select those with lowest p-values.

```{r, echo=FALSE}
#######################################################################
# The objects were created when doing experiments in the console.     #
# They've been saved to a file, so as not to waste time re-building.  #
# The following chunk contains the actual (creation) code.            #
#######################################################################
mnm <- readRDS("mnm.rds")
smry <- readRDS("smry.rds")
```
```{r feature.selection, eval=FALSE}
library(nnet) # for multinom[ial] model
mnm <- multinom(classe ~ ., data = tr)
smry <- summary(mnm)
```

<div class="side-note">
##### On linear model feature selection
Of course, lienear model selection is an art in and of itself. What we do
here is an oversimplified version of _backward stepwise selection_. 
What we later call "best" or "top" variables might be far from being good at all.
They do work all right in this particular case, i.e. they lead to
a highly accurate model of signifficantly reduced size.
</div>

We compute z-value, and associated p-values for each variable. We then order
the variables by p-values (smallest first).
```{r}
z <- smry$coefficients / smry$standard.errors
p <- (1 - pnorm(abs(z))) * 2; # 2-tail
p <- p[,-1]                   # ignore intercept
p <- sort(apply(p, 2, sum))   # best features first
```

<figure>
```{r bestfeatures, echo=FALSE}
library(lattice)
set.seed(12345)
intr <- createDataPartition(tr$classe, p=0.02, list=FALSE)
d <- tr[intr,]
splom(d[names(p[10:15])], groups=d$classe, varname.cex=.7, axis.text.cex=0)
```
  <figcaption>
  Fig. 2. The first five (of 20) variables that will be used
  to train the final model. Notice that we can already see small regions
  of homogenous color (`classe`). The plot was made using a 2% of the
  training data.
  </figcaption>
<figure>

## Training a random forest model
We choose 20 "best" variables and fit a random forest model.
```{r, echo=FALSE}
#######################################################################
# The objects were created when doing experiments in the console.     #
# They've been saved to a file, so as not to waste time re-building.  #
# The following chunk contains the actual (creation) code.            #
#######################################################################
mrf <- readRDS('mrf.rds')
```
```{r train, eval=FALSE}
mrf <- train(classe ~ ., 
             data = tr[, c('classe', names(p[1:20]))], # 20 'best' features
             method = "rf")
# Now go, make yourself a cup of coffee!
```
```{r, echo=FALSE}
mrf
```
<figure>
```{r densityplot, echo=FALSE,fig.width=8, fig.height=4}
print(densityplot(mrf), position=c(0, 0, .4, 1), more=T)
print(plot(varImp(mrf)), position=c(0.4, 0, 1, 1))
```
  <figcaption>
  Fig. 3. Left: within-model accuracy distribution. The expected value is 0.996.
  Right: variables' importance (mean decrease in node impurity scaled to 100,
  e.g. _mean decrease gini_ for `num_window` is
  `r round(randomForest::importance(mrf$finalModel)['num_window',], 2)`)
  </figcaption>
</figure>

<figure>
```{r errrate, echo=FALSE, fig.width=6, fig.height=4}
plot(mrf$finalModel, main="")
legend("topright", colnames(mrf$finalModel$err.rate), fill=1:6)
```
  <figcaption>
  Fig. 4. Error rate (per class and "out-of-bag") as a function of number
  of trees. It decreases quickly (until the number of trees is about 50),
  and then stabilizes below 0.01.
  </figcaption>
</figure>

## Results and summary
```{r}
confusionMatrix(predict(mrf, tst), tst$classe)
```

The accuracy of test set prediction is 0.9978. And, indeed, applied onto
the "true" test set (20 examples without `classe` label in `pml-training.csv')
the model managed 100% accuracy.

The random forest model is ineed a powerful technique. It is hardly surprising
that it was chosen as the default method of the `caret` packages's `train`
function. All other parameters were also left to their default values,
and the final model's good results are a proof of great knowledge
and experience of the package authors.

Simplified feature (pre)selection with multinomial regression was an experiment
aimed only at saving time. It did work well and seemingly did not much affect
the accuracy.

## References
