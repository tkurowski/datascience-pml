---
title: "Machine Learning for Qualitative Prediction of Weight Lifting Exercises"
author: "Tomasz Kurowski"
date: "May 9, 2016"
output: html_document
---

<style>
    blockquote {font-size: inherit; font-style: italic;}

    .side-note {
        font-size: 0.9em; padding: 1em 2em;
        border: solid 1px #ddd;
        border-width: 1px 0;
        margin: 2em 4em;
    }
    .side-note:before {
        content: "side note "; text-transform: uppercase;
        font-size: 0.8em;
    }
    .side-note h5 {
        font-size: 1em; font-weight: bold;
    }

    figure {font-style: italic; margin-bottom: 1em;}
</style>

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, fig.path='plots/',comment=NA,cache=TRUE,messages=F)
```

```{r, echo=FALSE}
# go to project dir
if (getwd() != "/home/tkurowski/Coursera/PML") setwd("~/Coursera/PML/")
```

## Executive Summary

Using [Qualitative Activity Recognition of Weight Lifting Exercises][dataset]
data a classification random forest is built for predicting quality
of weight lifting exercises from sensor measurements.

- the data is explored and cleaned
- preliminary feature selection is performed using multinomial regression
- random forest model is fit using [`caret` package][caret]
- and its performance is evaluated

The full `R` source code can be found at [project's github page](https://github.com/tkurowski/datascience-pml) and is considered an integral
part of this report.

## Background

Human Activity Recognition (HAR) uses and examines data generated by personal "wearable" computers: smartphones, fitness trackers, smartwatches. Potential
applications for HAR include: elderly monitoring, weight-loss programs support
or digital assistance for physical exercises.
Traditionally, HAR focused on identifying different types of activities;
little attention was paid to the quality of performing exercises.

Velloso, E. et. al. (2013) carried out a study in which

> Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
>
> Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

and using the gathered data they

> investigated qualitative activity recognition
> on the example of assessing the quality of execution of weight
> lifting  exercises.

They presented their results at the 4th Augmented Human (AH) International
Conference in 2013, in a research paper [Qualitative Activity Recognition
of Weight Lifting Exercises][dataset].

In this analysis we will use the same data set and build a classification model
for predicting `classe` dependent variable using (a subset of) over 150
independed variables collected with four different sensors.

## Getting and Exploring the Data

The data comes in two sets: `pml-training.csv` and `pml-testing.csv`
(without the dependent variable).

```{r, echo=FALSE}
for (fname in c("pml-training.csv", "pml-testing.csv"))
    if (!file.exists(fname)) {
        path <- paste("https://d396qusza40orc.cloudfront.net/predmachlearn", fname,
                  sep="/")
        print(fname)
        download.file(path, fname)
    }
```
```{r}
df <- read.csv("pml-training.csv")
```

The training data contains `r dim(df)[1]` records and `r dim(df)[2]` columns.

<figure>
```{r exploryplots, echo=FALSE, fig.align="center"}
library(caret)
library(lattice)

lvl.plt <- levelplot(table(df[,c('classe', 'user_name')]), outer=F,
                     main=list(label="Number of exercises", cex=.7))

set.seed(12345)
smpl <- createDataPartition(df$classe, p=0.02, list=FALSE)
d <- df[smpl, ]
pairs.plt <- splom(d[, c("yaw_belt", "magnet_belt_z", "magnet_dumbbell_z",
                         "num_window", "yaw_dumbbell") ],
                   groups=d$classe,
                   varname.cex=.7, axis.text.cex=0)

print(lvl.plt, position=c(0, 0, .5, 1), more=T)
print(pairs.plt, position=c(0.5, 0, 1, 1))
```
  <figcaption style="margin-top: -4em">
  **Fig. 1.** Left: The number of exercises and the proportions of `classe` values
  are not constant between users. Right: Some variables chosen to train
  the final model. Notice that we can already see small regions
  of homogeneous color (`classe`). The plot was made using a 2% sample of the data.
  </figcaption>
</figure>


```{r withna}
with.na <- sapply(names(df), function (n) sum(is.na(df[[n]]))) # count NA
with.na <- with.na[with.na > 0] # filter columns with NA
```

`r length(with.na)` columns in the data are `r round(min(with.na)/dim(df)[1], 3)*100`%
`NA` (missing values).

<div class="side-note">

##### Assessing the irrelevance of the (almost) empty columns

Being almost empty does not, by itself, make the columns useless
(what if the few available values happen to determine the `classe` perfectly?).
Figure 2 shows the classification tree built using the near-empty columns.

```{r rpart, echo=FALSE}
set.seed(123)
m.with.na <- train(classe ~ .,
                   data = df[, c('classe', names(with.na))],
                   method="rpart")
```

<figure>
```{r rpart_tree, echo=FALSE, fig.width=4, fig.height=2.6}
par('mar', 'cex') -> pars
par(mar=c(0,0,0,0), cex=.75)
plot(m.with.na$finalModel, margin=.2); text(m.with.na$finalModel)
par(pars)
```
  <figcaption>
  **Fig. 2.** Classification tree built with `rpart` on near-empty columns.
  </figcaption>
</figure>
Only two of them are of any use and the estimated accuracy is only 
`r round(m.with.na$results[1,2], 3)`.
Besides, the tree can only be applied to records where given columns
have proper values.
</div>

## Cleaning the Data

We discard irrelevant columns. These include date and time variables,
row number (`X`), `new_window` (yes-no factor that is "no" in
`r round(sum(df$new_window == 'no')/dim(df)[1] * 100)`% of cases) and `user_name`.

```{r irrelevant}
irrelevant <- c(names(with.na), 'X', 'user_name', 'new_window',
                'cvtd_timestamp', 'raw_timestamp_part_1', 'raw_timestamp_part_2')
df[irrelevant] <- NULL
```

<div class="side-note">
##### Is `user_name` (ir)relevant?

In theory, `user_name` might be a powerful decision variable for the data set.
However, this would yield a very narrow model, unusable for greater audience.
</div>

`read.csv` coerced many variables to factors. We bring them back to their
original numeric domain.

```{r clean}
# NOTE: this clean function must also be applied to the test set (for prediction)
clean <- function (df) {
    for (n in names(df))
        if (n != "classe" && class(df[[n]]) == 'factor')
            df[[n]] <- as.numeric(df[[n]])
    df
}
df <- clean(df)
```

## Partitioning

The `caret` package uses techniques like cross-validation or bagging to find
the best fit; we will additionally simulate a test set to better evaluate model's accuracy.

```{r partition}
set.seed(4321)
intr <- createDataPartition(df$classe, p=0.7, list=FALSE)
tr <- df[intr, ]
tst <- df[-intr, ]
```

## Preliminary Feature Selection

The training data now contains `r dim(tr)[1]` rows, `r length(tr)` columns each.
To train such a big data set (with `caret`'s built in machinery) one needs significant amount of time or computing power.
We will therefore perform a very simple feature selection that will, nevertheless,
turn out quite effective.

The idea is to fit a multinomial linear regression model using all variables
and then select those with lowest p-values.

```{r, echo=FALSE}
#######################################################################
# The objects were created when doing experiments in the console.     #
# They've been saved to a file, so as not to waste time re-building.  #
# The following chunk contains the actual (creation) code.            #
#######################################################################
smry <- readRDS("smry.rds")
```
```{r feature.selection, eval=FALSE}
library(nnet) # for multinom[ial] model
mnm <- multinom(classe ~ ., data = tr)
smry <- summary(mnm)
```

<div class="side-note">
##### On linear model feature selection
Of course, linear model selection is an art in and of itself. What we do
here is an oversimplified version of _backward stepwise selection_. 
What we later call "best" or "top" variables might be far from being good at all.
They do work all right in this particular case, i.e. they lead to
a highly accurate model of significantly reduced size.
</div>

We compute z-value, and associated p-values for each variable. We then order
the variables by p-values (smallest first).
```{r}
z <- smry$coefficients / smry$standard.errors
p <- (1 - pnorm(abs(z))) * 2; # 2-tail
p <- p[,-1]                   # ignore intercept
p <- sort(apply(p, 2, sum))   # best features first
```

The righ-hand panel of figure 1 presents a scatter plot matrix of 5 (of 20) variables that will be used to train the final model.

## Training a Random Forest Model
We choose 20 "best" variables and fit a random forest model.
```{r, echo=FALSE}
#######################################################################
# The objects were created when doing experiments in the console.     #
# They've been saved to a file, so as not to waste time re-building.  #
# The following chunk contains the actual (creation) code.            #
#######################################################################
mrf <- readRDS('mrf.rds')
```
```{r train, eval=FALSE}
mrf <- train(classe ~ ., 
             data = tr[, c('classe', names(p[1:20]))], # 20 'best' features
             method = "rf")
# Now go, make yourself a cup of coffee!
```
```{r, echo=FALSE}
mrf
```
<figure>
```{r densityplot, echo=FALSE,fig.width=8, fig.height=4, fig.align="center"}
print(densityplot(mrf), position=c(0, 0, .4, 1), more=T)
print(plot(varImp(mrf, scale=FALSE)), position=c(0.4, 0, 1, 1))
```
  <figcaption>
  **Fig. 3.** Left: within-model accuracy distribution. The expected value is 0.996.
  Right: variables' importance computed as the mean decrease in node impurity
  (gini index).
  </figcaption>
</figure>

<figure>
```{r errrate, echo=FALSE, fig.width=6, fig.height=4}
plot(mrf$finalModel, main="")
legend("topright", colnames(mrf$finalModel$err.rate), fill=1:6)
```
  <figcaption>
  **Fig. 4.** Error rate (per class and "out-of-bag") as a function of number
  of trees. It decreases quickly (until the number of trees is about 50),
  and then stabilizes below 0.01.
  </figcaption>
</figure>

## Testing the Model

```{r}
confusionMatrix(predict(mrf, tst), tst$classe)
```

The accuracy of test set prediction is 0.9978. And, indeed, applied onto
the "true" test set (20 examples without `classe` label in `pml-testing.csv`)
the model achieved 100% accuracy.

## Conclusion

_Random forest_ is ineed a powerful technique. It is hardly surprising
that it was chosen as the default method of the `caret` packages's `train`
function. All other parameters were also left to their default values,
and the final model's good results are proof of great knowledge
and experience of the package authors.

Simplified feature (pre)selection with multinomial regression was an experiment
aimed only at saving time. It did work well and seemingly did not (much) affect accuracy.

## References

1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises][dataset]. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013.
2. [`caret` package documentation][caret]
3. Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, [An Introduction to Statistical Learning][islr]

[dataset]: http://groupware.les.inf.puc-rio.br/work.jsf?p1=1120
[caret]: http://topepo.github.io/caret/index.html
[islr]: http://www-bcf.usc.edu/~gareth/ISL/
